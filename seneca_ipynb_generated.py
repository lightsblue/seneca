# -*- coding: utf-8 -*-
"""seneca.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XOkxCDssaVqaBbDvlR8Qc9fKQRFDmmvv

# Settings
"""

letter_index = 74 #71

"""# Install libs"""

!pip install openai fpdf
!pip install ebooklib

"""# Latin Letter Download"""

from typing import List, Optional
from dataclasses import dataclass
import requests
from bs4 import BeautifulSoup

@dataclass
class Letter:
    number: int
    roman: str
    title: str
    content: str

def download_content(url: str) -> str:
    response = requests.get(url)
    response.raise_for_status()
    return response.text

def roman_to_int(roman: str) -> int:
    roman = roman.upper()
    roman_numerals = {'I': 1, 'V': 5, 'X': 10, 'L': 50, 'C': 100, 'D': 500, 'M': 1000}
    result = 0
    prev_value = 0
    for c in reversed(roman):
        if c not in roman_numerals:
            raise ValueError(f"Invalid Roman numeral character: {c}")
        value = roman_numerals[c]
        if value < prev_value:
            result -= value
        else:
            result += value
            prev_value = value
    return result

def extract_letters(content: str) -> List[Letter]:
    soup = BeautifulSoup(content, 'html.parser')
    body = soup.find('body')
    letters: List[Letter] = []
    current_letter_number: Optional[int] = None
    current_letter_roman: Optional[str] = None
    current_letter_title: Optional[str] = None
    current_letter_content: List[str] = []
    collecting = False

    for tag in body.find_all('p'):
        b_tag = tag.find('b')
        if b_tag:
            if current_letter_number is not None:
                letters.append(Letter(
                    number=current_letter_number,
                    roman=current_letter_roman,
                    title=current_letter_title or "",
                    content='\n'.join(current_letter_content).strip()
                ))

            title_text = b_tag.get_text(strip=True)
            parts = title_text.split('.', 1)
            if len(parts) == 2:
                roman_part = parts[0].strip()
                try:
                    number = roman_to_int(roman_part)
                    title_remainder = parts[1].strip()
                    current_letter_number = number
                    current_letter_roman = roman_part
                    current_letter_title = title_remainder
                    current_letter_content = []
                    collecting = True
                except ValueError:
                    collecting = False
                    current_letter_number = None
                    current_letter_title = None
            else:
                collecting = False
        elif tag.get('class') and 'shortborder' in tag.get('class'):
            if current_letter_number is not None:
                letters.append(Letter(
                    number=current_letter_number,
                    roman=current_letter_roman,
                    title=current_letter_title or "",
                    content='\n'.join(current_letter_content).strip()
                ))
            current_letter_number = None
            current_letter_title = None
            current_letter_content = []
            collecting = False
        elif collecting:
            current_letter_content.append(tag.get_text())

    if current_letter_number is not None and current_letter_content:
        letters.append(Letter(
            number=current_letter_number,
            roman=current_letter_roman,
            title=current_letter_title or "",
            content='\n'.join(current_letter_content).strip()
        ))

    return letters

# List of URLs to process
urls = [
    "https://www.thelatinlibrary.com/sen/seneca.ep1.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep2.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep3.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep4.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep5.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep6.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep7.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep8.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep9.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep10.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep11-13.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep14-15.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep17-18.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep19.shtml",
    "https://www.thelatinlibrary.com/sen/seneca.ep20.shtml"
]

# Initialize an empty list to store all letters
all_letters: List[Letter] = []

# Process each URL and collect letters
for url in urls:
    print(f"Processing {url}")
    try:
        content = download_content(url)
        letters = extract_letters(content)
        # Extend all_letters with letters from this URL
        all_letters.extend(letters)
    except Exception as e:
        print(f"An error occurred while processing {url}: {e}")

# Now 'all_letters' contains all the letters from all the URLs
print(f"Total letters collected: {len(all_letters)}")

from typing import List, TypedDict

class ParagraphData(TypedDict):
    paragraph_index: int
    sentences: List[str]

import openai
import textwrap

from openai import OpenAI
from google.colab import userdata

client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))

def translate_letter(letter_text, instructions):
    completion = client.chat.completions.create(
        model='gpt-4o',
        messages=[
            {'role': 'system', 'content': instructions},
            {'role': 'user', 'content': letter_text}
        ],
        temperature=0.7
    )
    translation = completion.choices[0].message.content
    return translation

import re
import openai
import textwrap

from openai import OpenAI
from google.colab import userdata

client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))

def split_paragraphs(text):
    """Split the text on double newlines into paragraphs."""
    return [p.strip() for p in text.strip().split('\n\n') if p.strip()]

def split_text_with_quotes(text):
    """
    Split text on sentence-ending punctuation, but preserve everything
    within matching single or double quotes as a single chunk.
    """
    chunks = []
    current_chunk = []
    inside_quotes = False
    quote_char = None  # Tracks whether we opened a single/double quote

    for char in text:
        current_chunk.append(char)
        # Toggle quotes if encountering any quote characters.
        if char in ['"', '“', '”', "'"]:
            if not inside_quotes:
                inside_quotes = True
                quote_char = char
            else:
                if char == quote_char:
                    inside_quotes = False
                    quote_char = None
        # If we're not inside quotes, treat '.', '!', '?' as sentence boundaries.
        if not inside_quotes and char in ['.', '!', '?']:
            chunks.append(''.join(current_chunk).strip())
            current_chunk = []
    if current_chunk:
        leftover = ''.join(current_chunk).strip()
        if leftover:
            chunks.append(leftover)
    return chunks

def split_naive_sentences(text):
    """A simple sentence splitter that always splits on '.', '!', or '?'."""
    chunks = []
    current_chunk = []
    for char in text:
        current_chunk.append(char)
        if char in ['.', '!', '?']:
            chunks.append(''.join(current_chunk).strip())
            current_chunk = []
    if current_chunk:
        leftover = ''.join(current_chunk).strip()
        if leftover:
            chunks.append(leftover)
    return chunks

def extract_outer_quoted_parts(text):
    """
    If text contains at least one pair of matching quotes (either ' or "),
    extract and return a tuple of (prefix, quote_char, inner_text, suffix).
    For example, given:
       "Clamo: 'Avoid the crowd. Stay away.' Extra text"
    it returns:
       ("Clamo: ", "'", "Avoid the crowd. Stay away.", " Extra text")
    If no matching quotes are found, returns ("", None, text, "").
    """
    first_quote = re.search(r"['\"]", text)
    if first_quote:
        quote_char = text[first_quote.start()]
        last_quote = text.rfind(quote_char)
        if last_quote > first_quote.start():
            prefix = text[:first_quote.start()]
            inner = text[first_quote.start()+1:last_quote]
            suffix = text[last_quote+1:]
            return prefix, quote_char, inner, suffix
    return "", None, text, ""

def clean_translation(text, original_sentence=""):
    """
    Remove wrapping quotes added by the LLM, and optionally reattach any leading numbering.
    """
    if (text.startswith('"') and text.endswith('"')) or (text.startswith("'") and text.endswith("'")):
        text = text[1:-1].strip()
    m = re.match(r'^(\[\d+\])', original_sentence)
    if m and not text.startswith(m.group(1)):
        text = m.group(1) + " " + text
    return text

def translate_chunk(chunk, instructions, conversation_history, max_context=1):
    """
    Translates a single chunk using the chat API while maintaining conversation history.
    """
    conversation_history.append({"role": "user", "content": chunk})
    if len(conversation_history) > (max_context * 2 + 1):
        messages_to_send = [conversation_history[0]] + conversation_history[-(max_context * 2):]
    else:
        messages_to_send = conversation_history

    print(f"Translating chunk with context (sending {len(messages_to_send)} messages)...")
    completion = client.chat.completions.create(
        model='gpt-4o',
        messages=messages_to_send,
        temperature=0.7,
    )
    reply = completion.choices[0].message.content.strip()
    conversation_history.append({"role": "assistant", "content": reply})
    return reply, conversation_history

def process_letter(letter_text, instructions=None, max_context=1, translate=True, monologue_threshold=3):
    """
    Processes a letter by:
      - Splitting the text into paragraphs.
      - Splitting each paragraph into sentences using quote-aware splitting.
      - For each sentence:
          * If it is "large" (contains more than monologue_threshold punctuation marks)
            and contains matching outer quotes, split it into three parts:
                prefix (before first quote),
                the quoted inner text,
                suffix (after the last quote).
            Translate each part separately:
                - For the quoted inner text, further split into subsentences and process each.
            Then reassemble: prefix_translation + quote_char + inner_translation + quote_char + suffix_translation.
          * Otherwise, process (translate) it normally.

    Returns a list of paragraphs. Each paragraph is a dict:
      {
         "paragraph_index": <int>,
         "sentences": [ <sentence 1>, <sentence 2>, ... ]
      }
    """
    paragraphs = split_paragraphs(letter_text)
    output_paragraphs = []
    conversation_history = [{"role": "system", "content": instructions}] if translate and instructions else []

    for para_idx, paragraph in enumerate(paragraphs, start=1):
        sentence_list = []
        sentences = split_text_with_quotes(paragraph)
        for sentence in sentences:
            punct_count = sentence.count('.') + sentence.count('!') + sentence.count('?')
            if punct_count > monologue_threshold:
                print(f"Large sentence detected: {sentence}")
                prefix, quote_char, inner_sentence, suffix = extract_outer_quoted_parts(sentence)
                if quote_char:
                    working_sentence = inner_sentence
                else:
                    working_sentence = sentence

                # Process the inner part by splitting into subsentences.
                subsentences = split_naive_sentences(working_sentence)
                sub_translations = []
                for subsentence in subsentences:
                    if translate:
                        prompt = f"Translate this sentence:\n\n{subsentence}"
                        sub_translation, conversation_history = translate_chunk(prompt, instructions, conversation_history, max_context)
                        sub_translation = clean_translation(sub_translation, original_sentence=subsentence)
                    else:
                        sub_translation = subsentence
                    sub_translations.append(sub_translation)
                inner_translation = " ".join(sub_translations)
                inner_translation = clean_translation(inner_translation, original_sentence=working_sentence)
                # Translate prefix and suffix normally.
                if prefix.strip():
                    if translate:
                        prompt = f"Translate this text:\n\n{prefix}"
                        prefix_translation, conversation_history = translate_chunk(prompt, instructions, conversation_history, max_context)
                        prefix_translation = clean_translation(prefix_translation, original_sentence=prefix)
                    else:
                        prefix_translation = prefix
                else:
                    prefix_translation = ""
                if suffix.strip():
                    if translate:
                        prompt = f"Translate this text:\n\n{suffix}"
                        suffix_translation, conversation_history = translate_chunk(prompt, instructions, conversation_history, max_context)
                        suffix_translation = clean_translation(suffix_translation, original_sentence=suffix)
                    else:
                        suffix_translation = suffix
                else:
                    suffix_translation = ""
                # Reassemble: rewrap inner translation with quotes if applicable.
                if quote_char:
                    translation = f"{prefix_translation}{quote_char}{inner_translation}{quote_char}{suffix_translation}"
                else:
                    translation = f"{prefix_translation}{inner_translation}{suffix_translation}"
                translation = clean_translation(translation, original_sentence=sentence)
            else:
                if translate:
                    prompt = f"Translate this sentence:\n\n{sentence}"
                    translation, conversation_history = translate_chunk(prompt, instructions, conversation_history, max_context)
                    translation = clean_translation(translation, original_sentence=sentence)
                else:
                    translation = sentence
            sentence_list.append(translation)
        output_paragraphs.append({
            "paragraph_index": para_idx,
            "sentences": sentence_list
        })

    return output_paragraphs

def reconstruct_translation(translation_data):
    """
    Given a list of paragraphs (each a dict with a "sentences" array),
    reconstruct the final translated text by joining sentences within
    each paragraph (with a space) and paragraphs (with two newlines).

    Parameters:
      translation_data: list of dicts, where each dict has keys:
                        - "paragraph_index": int
                        - "sentences": list of translated sentences (str)

    Returns:
      A single string with the reconstructed text.
    """
    paragraphs = []
    for para in translation_data:
        # Join sentences with a space.
        para_text = " ".join(para["sentences"]).strip()
        paragraphs.append(para_text)
    # Join paragraphs with two newlines.
    return "\n\n".join(paragraphs)

# Example usage:
# Assume `result` is the output from translate_letter_by_paragraph_section_sentence().
# final_text = reconstruct_translation(result)
# print(final_text)

import re

def parse_letter(letter: str) -> List[ParagraphData]:
    """
    Parses a raw text letter from Seneca into a structured list of paragraphs.

    Each paragraph in the returned list is represented as a dictionary with:
      - "paragraph_index": <int>
      - "sentences": [<sentence1>, <sentence2>, ...]

    Paragraphs are assumed to be separated by two or more newlines.
    Sentences are split using punctuation (. ! ?) followed by whitespace.
    """
    structured_data: List[ParagraphData] = []
    # Split the raw letter into paragraphs (ignoring empty paragraphs)
    raw_paragraphs: List[str] = [p.strip() for p in letter.split("\n\n") if p.strip()]

    for idx, paragraph in enumerate(raw_paragraphs, start=1):
        # Split the paragraph into sentences.
        # This regex looks for punctuation (.!?), then one or more whitespace characters.
        sentences: List[str] = [s.strip() for s in re.split(r'(?<=[.!?])\s+', paragraph) if s.strip()]
        structured_data.append({
            "paragraph_index": idx,
            "sentences": sentences
        })

    return structured_data

import re
from typing import List, TypedDict
from IPython.display import Markdown, display

def compare_translation_data(*datasets: List[ParagraphData]) -> str:
    """
    Compares multiple translation data structures and returns a Markdown string.

    Each data structure is a list of paragraphs, where each paragraph is represented as a ParagraphData:
      {
         "paragraph_index": <int>,
         "sentences": [ <translated sentence 1>, <translated sentence 2>, ... ]
      }

    The function compares paragraphs by index, and for each paragraph, compares
    the sentences one-by-one across all provided translations.
    """
    md_str: str = ""
    # Determine the maximum number of paragraphs across all datasets.
    num_paras: int = max(len(data) for data in datasets)

    for i in range(num_paras):
        md_str += f"## Paragraph {i+1}\n\n"
        # For each dataset, get the paragraph if it exists, otherwise assume an empty list.
        paragraphs: List[ParagraphData] = [
            data[i] if i < len(data) else {"sentences": []} for data in datasets
        ]
        # Get the sentences list for each dataset.
        sentence_lists: List[List[str]] = [para.get("sentences", []) for para in paragraphs]
        # Determine the maximum number of sentences in this paragraph.
        max_sentences: int = max(len(sentences) for sentences in sentence_lists)

        for j in range(max_sentences):
            md_str += f"**Sentence {j+1}:**\n\n"
            for idx, sentences in enumerate(sentence_lists, start=1):
                sentence: str = sentences[j] if j < len(sentences) else ""
                md_str += f"- {idx}: `{sentence}`\n"
            md_str += "\n"
        md_str += "---\n\n"

    return md_str

# Example usage:
# Assume translation_data1, translation_data2, etc. are lists of ParagraphData.
# md_comparison = compare_translation_data(translation_data1, translation_data2, translation_data3)
# display(Markdown(md_comparison))

from IPython.display import display, Markdown
letter:Letter = all_letters[letter_index]
display(Markdown(f"**{letter.roman} ({letter.number}) {letter.title}**\n\n{letter.content}"))

"""# 2 Phase Translation"""

#test_text_p = process_letter(letter.content, translate=False)
#test_text_p
#print(test_text_p[0].get("sentences")[0])

direct_rewrite_prompt = """
You are a Latinist and philosopher working sentence by sentence through Seneca’s writing. Your task is to produce a close, literal, word-for-word English translation of each Latin sentence. This translation should preserve the Latin syntax and logic as closely as possible. It is not for publication—it will serve as the foundation for a second-stage rewrite into modern English.

Your goals:

1. **Preserve Latin structure and clause sequence.**
   - Translate each Latin clause in the order it appears, maintaining clause order and subordination structure.
   - Retain subject-object-verb relationships unless the result would be ungrammatical in English.
   - Preserve rhetorical devices (e.g. triads, contrasts, reversals) and clause relationships exactly as they appear in the Latin.
   - Do not rearrange or combine clauses for clarity or flow.

2. **Use clean, consistent, grammatically complete literal English.**
   - Use simple, modern English words that match Latin meaning precisely. Avoid idiomatic phrasing or stylistic smoothing.
   - Maintain consistent translation for repeated Latin terms across the passage (e.g., *res* → “affair”, *consilium* → “counsel”).
   - Choose vivid and active verb equivalents when appropriate (e.g., *volvuntur* → “are tossed” or “are thrown about,” not “are rolled”).
   - Preserve tense, mood, and voice exactly (e.g., future perfect, subjunctive, passive voice).
   - Passive constructions should remain passive in English unless absolutely ungrammatical.
   - Use adjectival forms where the Latin uses adjectives (e.g., *honestum* → “virtuous,” not “virtue” or “the virtuous,” unless clearly used as a noun).
   - Ensure idioms and literal phrases remain faithful **but are still grammatically complete and unambiguous in English**. If a phrase would be confusing or meaningless to a modern reader, preserve the structure while slightly clarifying word choice—without interpretation.
   - For directional or destination phrases (e.g., *quo derigantur*), preserve the Latin structure while using English phrasing that clearly implies purpose or aim (e.g., “the target toward which they are directed” rather than “where they are directed”).

3. **Translate Stoic terms with philosophical precision.**
   - Use the following consistent translations:
     - *honestum* → “virtuous” (or “morally good” when used adjectivally)
     - *virtus* → “virtue” or “moral excellence”
     - *summum bonum* → “the highest good”
     - *adulterina bona* → “counterfeit goods” or “false goods”
   - Do not substitute philosophical terms with tone-based or generic synonyms (e.g., avoid “integrity,” “honor,” “goodness”).

4. **Avoid paraphrasing or interpretation.**
   - Translate literally and analytically, without smoothing, summarizing, or rewording.
   - Do not infer meaning or insert connective logic not found in the Latin.
   - Break up long Latin sentences into shorter English clauses only if doing so mirrors the Latin structure—not for readability or tone.

5. **Prioritize physical, concrete translations for words with multiple meanings.**
   - Translate nouns like fundus using their literal, tangible meaning (e.g., “bottom” of a vessel) unless the sentence clearly and explicitly refers to land, property, or abstract domains.
   - When in doubt, prefer the meaning that preserves a physical, image-based metaphor.

6. **Identify and preserve original imagery in metaphors and proverbs.**
   - Maintain vivid physical images (like containers, vessels, or physical places) as they appear in the Latin, even when modern readers might miss the implied meaning.
   - In proverbs, always preserve the metaphor’s physical imagery rather than substituting with abstract or metaphorically extended terms like “estate” or “farm.”
   - Avoid replacing concrete terms with conceptual summaries.

7. **No explanation, notes, or formatting.**
   - Do not include commentary, paraphrases, definitions, or interpretive guidance.
   - Return only the literal English translation in correct and unembellished prose.
   - Each sentence should map directly and transparently to its Latin source.

This step is intended to produce a grammatically correct but strictly literal scaffold for a separate rhetorical rewrite.
"""

direct_rewrite = process_letter(letter.content, direct_rewrite_prompt, max_context=2)
direct_rewrite

display(Markdown(f"**OpenAI Translation**:\n\n{reconstruct_translation(direct_rewrite)}"))

rhetorical_rewrite_prompt_v1 = """
You are a writer and philosopher translating Seneca for a modern audience. Your source material is a literal, clause-by-clause English translation from the original Latin. Your task is to rewrite this into clear, flowing, modern English—without softening Seneca’s force, precision, or Stoic clarity.

Imagine Seneca is writing for 2025: an intelligent, reflective audience that reads Substack essays and listens to serious podcasts. The tone should be urgent, grounded, and exacting. No fluff, no corporate polish, no academic hedging. Think: timeless moral clarity expressed in crisp, contemporary prose.

Your goals:

1. **Clarity and force.**
   - Rewrite in clear, sharp, modern English.
   - Eliminate awkward or overly literal phrasing from the original translation.
   - Avoid old-fashioned or formal words (“thus,” “one ought,” “wherein,” “perish”).
   - Use strong, modern verbs (“face,” “waste,” “own,” “avoid”) and natural phrasing.

2. **Preserve the structure of Seneca’s reasoning.**
   - Keep triads, contrasts, and rhetorical turns intact.
   - Where Latin builds through repetition or reversal, retain that rhythm in modern style.
   - Preserve punchy final lines or shifts in tone—these are often where the message lands hardest.

3. **Let the judgment bite.**
   - Do not neutralize moral clarity. If Seneca is making a hard claim about what matters in life, keep it hard.
   - Be bold with aphorisms, lines that could live on a wall or headline a post.
   - Do not water down his critiques of human behavior.

4. **Sound like someone speaking to us today.**
   - No academic footnotes. No flowery translation-speak. No corporate tone.
   - Seneca should sound timeless and alive, like someone who understands modern distraction and still insists on virtue.
   - The tone should feel like a beautifully written essay, not a textbook.

5. **Do not add or invent.**
   - Stay tightly anchored to the meaning and structure of the literal translation.
   - You may clarify a phrase when needed, but never add new ideas or interpretations not found in the Latin.

6. **Be inclusive, not performative.**
   - Use gender-neutral language where it preserves clarity and tone.
   - Avoid gendered generics like “he who” or “man” unless the Latin unambiguously intends it and it matters for the reasoning.

Begin rewriting from the literal translation, sentence by sentence. Maintain a cohesive, flowing voice.
"""

rhetorical_rewrite_prompt_v2 = """
You are a writer and philosopher translating Seneca for a modern audience. Your source is a literal English translation from the original Latin. Rewrite into clear, modern English without softening Seneca’s precision, Stoic clarity, or philosophical density.

Your goals:

1. **Clarity without dilution:**
   - Modernize wording, not ideas.
   - Preserve Seneca’s exact philosophical terms (idleness, virtue, suspicion, etc.)—avoid casual synonyms unless fully accurate.

2. **Maintain rhetorical precision:**
   - Explicitly keep triads, contrasts, parallel structures from Latin intact.
   - Preserve sharpness and bite of moral judgments and aphorisms. Do not water them down.

3. **Minimal structural adjustments:**
   - Slight clause-order adjustments allowed only when absolutely necessary for clarity, clearly indicated.

4. **Clear, timeless tone:**
   - Modern yet precise voice. Not overly casual. Imagine writing for a serious, reflective 2025 audience. No corporate fluff, no academic jargon.

5. **No added interpretation:**
   - Do not add ideas not explicitly present in original translation.
   - Retain Seneca’s imagery and concrete metaphors fully intact.

6. **Gender-inclusive clarity:**
   - Use inclusive language where consistent with Seneca’s intent, without changing philosophical meaning.

Rewrite sentence-by-sentence, staying tightly anchored to the literal translation provided.
"""

rhetorical_rewrite = process_letter(reconstruct_translation(direct_rewrite), rhetorical_rewrite_prompt_v1, max_context=2)
rhetorical_rewrite

md_comparison = compare_translation_data(process_letter(letter.content, translate=False), direct_rewrite, rhetorical_rewrite)
display(Markdown(md_comparison))

"""# General letter parsing for book based translation

"""

#direct_rewrite = process_letter(letter.content, direct_rewrite_prompt, max_context=2)
#rhetorical_rewrite = process_letter(reconstruct_translation(direct_rewrite), rhetorical_rewrite_prompt, max_context=2)

#display(Markdown(f"**OpenAI Translation**:\n\n{reconstruct_translation(rhetorical_rewrite)}"))

"""# Compare"""

#md_comparison = compare_translation_data(richard_m_gummere_translation_1, rhetorical_rewrite)
#display(Markdown(md_comparison))

"""# Creating Artifacts"""

# Install the ebooklib library
!pip install ebooklib

from ebooklib import epub
from google.colab import files
from datetime import datetime


# Assuming 'translation' is the translated text of Letter 81
# translation = ...

# Process the translation to replace newlines with <br/>
translation_html = reconstruct_translation(rhetorical_rewrite).replace("\n", "<br/>")

# Create a new EPUB book
book = epub.EpubBook()

# Format current time as "May 5th 12:11p"
now = datetime.now()
day = now.day
suffix = "th" if 11 <= day <= 13 else {1: "st", 2: "nd", 3: "rd"}.get(day % 10, "th")
formatted_time = now.strftime(f"%B {day}{suffix} %I:%M%p").lower().replace("am", "a").replace("pm", "p")

# Set title with readable datetime
book.set_title(f"Seneca's Letters – {formatted_time}")

# Set metadata
book.set_identifier('id123456')
book.set_language('en')
book.add_author('Seneca')  # Replace with your name or preferred author name

# Create a chapter for Letter 81
chapter = epub.EpubHtml(title=f"Letter {letter.number} {letter.title}", file_name=f"letter_{letter.number}.xhtml", lang='en')
chapter.content = f'<h1>Letter {letter.number}</h1><p>{translation_html}</p>'

# Add the chapter to the book
book.add_item(chapter)

# Define Table of Contents
book.toc = (epub.Link(f'letter_{letter.number}.xhtml', f'Letter {letter.number}', f'chap_{letter.number}'),)

# Add default NCX and Nav files
book.add_item(epub.EpubNcx())
book.add_item(epub.EpubNav())

# Define CSS style
style = '''
@namespace epub "http://www.idpf.org/2007/ops";
body {
    font-family: Times, serif;
    margin-left: 20px;
    margin-right: 20px;
}
h1 {
    text-align: center;
}
p {
    text-align: justify;
}
'''

# Add CSS file to the book
nav_css = epub.EpubItem(uid="style_nav", file_name="style/nav.css", media_type="text/css", content=style)
book.add_item(nav_css)

# Set the spine (order of content)
book.spine = ['nav', chapter]

# Save the EPUB to a file
epub_output_path = f'/content/translation_{letter.number}.epub'
epub.write_epub(epub_output_path, book, {})

# Provide a link to download the EPUB file
files.download(epub_output_path)